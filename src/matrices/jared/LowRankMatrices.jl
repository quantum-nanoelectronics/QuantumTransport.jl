module LowRankMatrices
    using LinearAlgebra
    using SparseArrays
    using ArnoldiMethod
    using Arpack

    export LRA
    # can likely pull these functions in from elsewhere
    norm_error(A,B) = norm( A - B ) / norm(A)
    
    #= in -> sparse, square matrix of value type ComplexF64 =#
    struct LRA <: AbstractMatrix{Complex}
        Rvecs :: Matrix{ComplexF64}
        Lvecs :: Matrix{ComplexF64}
        Î»s    :: Array{ComplexF64}
        rank  :: Int64
        n     :: Int64
        eigendecomposed :: Bool
        # only use this initializer with Hermitian sparse matrices
        function LRA(A::Union{SparseMatrixCSC{ComplexF64,Int64},SparseMatrixCSC{Float64,Int64}},
                Emin::ð‘, Emax::ð‘; 
                initialrank::Int = 64, rankstep::Int = 32,
                maxiters::Int = 20, errortol::ð‘ = 1e-2) where ð‘ <: Real
            if A.n != A.m
                throw(DomainError(A, "LRA DEFINED FOR SQUARE MATRICES ONLY."))
            end
            #= Only normal matrices are supported s.t. vâ‚— = váµ£* =#
            # TODO: add in a check for hermitian matrices specifically
            #= TODO: For hermitian matrices, implement Lanczos method =#
            #=
             if norm_error(A * conj(A) , conj(A) * A) > errortol
                throw(DomainError(A, "LRA NOT DEFINED FOR NON NORMAL MATRIX YET"))
            end
            =# 
            #= Ignore small matrices. Decrease INITIAL_RANK for testing. =#
            #=
            if A.n < initialrank
                return A
            end
            =#
            Î»s = undef; Rvecs = undef;
            curr_rank = initialrank
            success = false
            iters = 0
            centralenergy = (Emax+Emin)/2
            shiftedA = A - centralenergy*I(A.n)
            energywindow = Emax-Emin
            #= 
             Two stop conditions: number of iterations exceeds allowed maxiumum or
             eigen-energies are sufficiently close to defined window and the normalized
             error is small. In case maximum iterations achieved, will return original matrix.
            =#

            #= NOTE: second condition unnecessary, but useful for debugging small matrices =#
            while iters <= maxiters && curr_rank <= A.n
                # this needs to be changed, SR doesn't work -- needs SM, which is not implemented in arnoldi.jl
                # it is implemented in ARPACK.jl though
                #=schur, hist = partialschur(shiftedA, nev = curr_rank, tol = sqrt(eps()), which = ArnoldiMethod.SR())
                Î»s, Rvecs = partialeigen(schur) =#
                error = 0
                #error = norm_error(A, reconstruct(Rvecs, conj(Rvecs), Î»s)) #NOTE: expensive
                #println(error)
                Î»s, Rvecs = eigs(shiftedA, maxiter=1000, nev = curr_rank, which=:SM)
                #Î»s, Rvecs = eigs(shiftedA, maxiter=1000, nev = curr_rank, tol = sqrt(eps()), which=:SM)
                minÎ» = minimum(real.(Î»s))
                maxÎ» = maximum(real.(Î»s))
                # increase ARPACK decreases # eigvals
                curr_rank = length(Î»s)
                println("Rank = $curr_rank, Min Î» = $minÎ», Max Î» = $maxÎ», Energy window = $energywindow")
                if minÎ» < - energywindow/2 || maxÎ» > energywindow/2 
                    success = true
                    break
                end
                # normalize the phase on the first index of each eigvec
                #=if abs(Emin - minÎ») < Î”E &&
                   abs(Emax - maxÎ») < Î”E &&
                   error < errortol
                    success = true
                    break
                end=#
                iters+=1; curr_rank+=rankstep
            end
            #if success return new(Rvecs, conj(Rvecs), Î»s, curr_rank, A.n) else return A end
            return new(Rvecs, Array(Rvecs'), Î»s.+centralenergy, curr_rank, A.n, true)
        end

        function LRA(Rvecs::Matrix{ComplexF64}, Lvecs::Matrix{ComplexF64},
                     Î»s::Array{ComplexF64}, rank::Int64, n::Int64, eigendecomp::Bool=false)
            return new(Rvecs, Lvecs, Î»s, rank, n, eigendecomp)
        end

    end

    
    function Base.show(A::LRA)
        n = A.n
        println("$nÃ—$n Low Rank Matrix of rank $(A.rank)")
        println("Eigenvalues: $(A.Î»s)")
        println("Size of left eigenvector block: $(size(A.Lvecs))")
        println("Size of right eigenvector block: $(size(A.Rvecs))")
    end

    function Base.show(io::IO, A::LRA)
        n = A.n
        println("$nÃ—$n Low Rank Matrix of rank $(A.rank)")
        println("Eigenvalues: $(A.Î»s)")
        println("Size of left eigenvector block: $(size(A.Lvecs))")
        println("Size of right eigenvector block: $(size(A.Rvecs))")
    end

    # world ending critical failure if reconstruct. reconstruct expects normal matrix
    function reconstruct(A::LRA)
        @warn "You probably don't want to reconstruct a dense matrix. Reconsider, and do not use with large matrices."
        return A.Rvecs*Diagonal(A.Î»s)*A.Lvecs
    end

    function reconstruct(Rvecs::Matrix{ComplexF64}, Lvecs::Union{Matrix{ComplexF64}, Transpose{ComplexF64}},
                         Î»s::Array{ComplexF64})
        @warn "You probably don't want to do this. Reconsider, and do not use with large matrices."
        return Rvecs*Diagonal(A.Î»s)*Lvecs
    end

    function getindex_LRA(A::LRA, i::Int, j::Int)
        return A.Rvecs[i,:]*Diagonal(A.Î»s)*A.Lvecs[:,j]
    end

    function trace(A::LRA)
        sum = undef
        if A.eigendecomposed==true
            return sum(A.Î»s)
        else
            sum = 0 
            for i = 1:A.n
                sum += getindex_LRA(A,i,i)
            end
        end
        return sum
    end
    # TODO: fix, not a high priority though. 
    function LRAbyLRA(A::LRA, B::LRA)
        if A.n != B.n
            throw(DomainError(A, "BAD SHAPE IN LRA MATRIX MULTIPLICATION"))
        end
        # we wish to get Rá´¬Î›á´¬Lá´¬*Rá´®Î›á´®Lá´® into the form Rá¶œÎ›á¶œLá¶œ = Rá´¬*RPá¶œ*Î›á¶œ*LPá¶œ*Lá´®
        Î›á´¬Lá´¬Rá´®Î›á´® = Diagonal(A.Î»s)*A.Lvecs*B.Rvecs*Diagonal(B.Î»s)
        # this is done with inefficient, ill-conditioned jank because julia does not implement a left eigvec calculation. 
        F = eigen(Î›á´¬Lá´¬Rá´®Î›á´®); Î»sá¶œ = F.values; RPá¶œ = F.vectors
        LPá¶œ = Base.inv(RPá¶œ)   
        Rá¶œ = A.Rvecs*RPá¶œ; Lá¶œ = LPá¶œ*B.Lvecs
        return LRA(Rá¶œ,Lá¶œ,Î»sá¶œ,B.rank, B.n, true)
    end
#=    function LRAbyLRA(Aâ€²::LRA, Bâ€²::LRA)
        if Aâ€².n != Bâ€².n
            throw(DomainError(Aâ€², "BAD SHAPE IN LRA MATRIX MULTIPLICATION"))
        end
        Î© = zeros(ComplexF64, Bâ€².n, Bâ€².rank)   
        Ï‰ = Matrix{ComplexF64}(undef, Bâ€².n, Bâ€².rank)  
        Î»s = Matrix{ComplexF64}(undef, Bâ€².rank, 1)   
        for j in eachindex(Bâ€².Î»s)
            for i in eachindex(Aâ€².Î»s)
                Î©[:,j] += (Aâ€².Î»s[i] * Bâ€².Î»s[j] * Aâ€².Lvecs[:,i]' * Bâ€².Rvecs[:,j]) * Aâ€².Rvecs[:,i] 
            end
            #n = dot(conj(transpose(Î©[:,j])), Î©[:,j])
            n = Î©[:,j]' * Î©[:,j]
            Î»s[j] = n
            Ï‰[:,j] = Î©[:,j] / n
        end
        Câ€² = LRA(Ï‰, Bâ€².Lvecs, Î»s, Bâ€².rank, Bâ€².n)
        return Câ€²
    end=#


    function LRAbyMatrix(A::LRA, B::M) where M <: AbstractMatrix
        if A.n != size(A)[2]
            throw(DomainError(A, "BAD SHAPE IN LRA MATRIX MULTIPLICATION"))
        end
        #NOTE: Assumes A is linear
        C = LRA(B*A.Rvecs, A.Lvecs, A.Î»s, A.rank, A.n, false)
        return C
    end

    #TODO: maybe a nice operator representation of this?
    function mul_ind_LRA(Aâ€²::LRA, Bâ€²::LRA, i::Int, j::Int)
        return Aâ€²[i, :] â‹… Bâ€²[:,j]
    end

    function add_scaled_identity(D::Diagonal, A::LRA)
        D_1 = D[1,1]
        for i in 1:size(D)[1]
            if D[i,i] != D_1
                return perturbative_sum(A::LRA, D::Diagonal)
            end
        end
        return LRA(deepcopy(A.Rvecs), deepcopy(A.Lvecs), deepcopy(A.Î»s).+D_1, deepcopy(A.rank), deepcopy(A.n), deepcopy(A.eigendecomposed))
    end

    function inv(A::LRA)
        invÎ»s = (A.Î»s).^(-1)
        return LRA(A.Rvecs, A.Lvecs, invÎ»s, A.rank, A.n)
    end

    function perturbative_sum(A::LRA, B::M) where M <: AbstractMatrix 
        # now do non-hermitian degenerate perturbation theory
        return perturbative_sum(A,(B))
        return LRA(newRvecs, newLvecs, Î»s, A.rank, A.n, true)
    end


    function perturbative_sum(A::LRA, B::M...) where M <: AbstractMatrix 
        # now do non-hermitian degenerate perturbation theory
        Heff = zeros(ComplexF64,A.rank,A.rank)
        Heff += Diagonal(A.Î»s)
        for Î´H âˆˆ B 
            if typeof(Î´H) == LRA
                Test = A.Lvecs*Î´H*A.Rvecs
                Heff += A.Lvecs*Î´H.Rvecs*Diagonal(Î´H.Î»s)*Î´H.Lvecs*A.Rvecs
            else
                Test = A.Lvecs*Î´H*A.Rvecs
                Heff += Test
            end
        end
        # Rcoeffs and Lcoeffs are rank Ã— n matrices

        F = eigen(Heff)
        Î»s = F.values; Rcoeffs = F.vectors
        Lcoeffs = Base.inv(Rcoeffs)
        newRvecs = A.Rvecs*Rcoeffs
        newLvecs = Lcoeffs*A.Lvecs
        return LRA(newRvecs, newLvecs, Î»s, A.rank, A.n, true)
    end

    function number_multiplication(A::LRA, C::Number)
        return LRA(A.Rvecs, A.Lvecs, C*A.Î»s, A.rank, A.n)
    end

    function LRA_vector_multiplication(A::LRA, V::Vector)
        # assert size of vector same as A
        return A.Rvecs*(Diagonal(A.Î»s)*(A.Lvecs*V))
    end
 
    function LRA_vector_multiplication(V::Vector, A::LRA)
        # assert size of vector same as A
        return ((V*A.Rvecs)*Diagonal(A.Î»s))*A.Lvecs
    end
    
    Base.:size(Aâ€²::LRA) = (Aâ€².n, Aâ€².n)
    Base.:getindex(Aâ€²::LRA, i::Int, j::Int) = getindex_LRA(Aâ€²::LRA, i::Int, j::Int)
    
    #TODO: define rule for conversion?
    #Base.:promote_rule(...)
    #Base.:convert(...)

    # TODO: define shifting of eigenenergies when add identity matrix
    # TODO: define addition

    #Base.:+(Aâ€²::LRA, A::T) where T<:Matrix = +(promote(Aâ€²,A)...)
    #Base.:-(Aâ€²::LRA, A::T) where T<:Matrix = -(promote(Aâ€²,A)...)
    #Base.:+(Aâ€²::LRA, Bâ€²::LRA) = +(reconstruct(Aâ€²), reconstruct(Bâ€²))
    #Base.:-(Aâ€²::LRA, Bâ€²::LRA) = -(reconstruct(Aâ€²), reconstruct(Bâ€²))
    Array(A::LRA) = reconstruct(A)
    Base.:+(D::Diagonal, A::LRA) = add_scaled_identity(D,A)
    Base.:+(A::LRA, B::AbstractMatrix...) = perturbative_sum(A, B)
    Base.:+(A::LRA, D::Diagonal) = add_scaled_identity(D,A)
    Base.:+(A::LRA,B::LRA) = perturbative_sum(A,B)
    Base.:inv(A::LRA) = inv(A)
    Base.display(A::LRA) = show(A)

    Base.:*(V::Vector,Aâ€²::LRA) = LRA_vector_multiplication(V, Aâ€²)  
    Base.:*(Aâ€²::LRA,V::Vector) = LRA_vector_multiplication(Aâ€², V)
    Base.:*(Aâ€²::LRA,C::Number) = number_multiplication(Aâ€², C)
    Base.:*(C::Number, Aâ€²::LRA) = number_multiplication(Aâ€², C)
    Base.:*(Aâ€²::LRA, A::Union{SparseMatrixCSC{ComplexF64, Int64}, Matrix{ComplexF64}}) = LRAbyMatrix(Aâ€², A)
    Base.:*(A::Union{SparseMatrixCSC{ComplexF64, Int64}, Matrix{ComplexF64}}, Aâ€²::LRA) = LRAbyMatrix(Aâ€², A)
    Base.:*(Aâ€²::LRA, Bâ€²::LRA) = LRAbyLRA(Aâ€², Bâ€²)
end